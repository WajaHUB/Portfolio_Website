{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kTKaVK47knP"
   },
   "source": [
    "# Lab 8 Neural Language Model\n",
    "A language model predicts the next word in the sequence based on the specific words that have come before it in the sequence.\n",
    "\n",
    "It is also possible to develop language models at the character level using neural networks. The benefit of character-based language models is their small vocabulary and flexibility in handling any words, punctuation, and other document structure. This comes at the cost of requiring larger models that are slower to train.\n",
    "\n",
    "Nevertheless, in the field of neural language models, character-based models offer a lot of promise for a general, flexible and powerful approach to language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QK2Ew8_C7knQ"
   },
   "source": [
    "As a prerequisite for the lab, make sure to pip install:\n",
    "- keras\n",
    "- tensorflow\n",
    "- h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIs5eFI97knR"
   },
   "source": [
    "# Source Text Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQku8-L97knT"
   },
   "source": [
    "To start out with, we'll be using a simple nursery rhyme. It's quite short so we can actually train something on your CPU and see relatively interesting results. Please copy and paste the following text in a text file and save it as \"rhymes.txt\". Place this in the same directory as this jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wl1ZLGG7wii",
    "outputId": "46ae9912-604b-4a69-9c77-ddfda6729432",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\jackf\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.2)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: keras in c:\\users\\jackf\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\jackf\\anaconda3\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\jackf\\anaconda3\\lib\\site-packages (from h5py) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iqQdG7M68HVt"
   },
   "outputs": [],
   "source": [
    "s='Sing a song of sixpence,\\\n",
    "A pocket full of rye.\\\n",
    "Four and twenty blackbirds,\\\n",
    "Baked in a pie.\\\n",
    "When the pie was opened\\\n",
    "The birds began to sing;\\\n",
    "Wasn’t that a dainty dish,\\\n",
    "To set before the king.\\\n",
    "The king was in his counting house,\\\n",
    "Counting out his money;\\\n",
    "The queen was in the parlour,\\\n",
    "Eating bread and honey.\\\n",
    "The maid was in the garden,\\\n",
    "Hanging out the clothes,\\\n",
    "When down came a blackbird\\\n",
    "And pecked off her nose.'\n",
    "\n",
    "with open('rhymes.txt','w') as f:\n",
    "  f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq7wq3fF7knV"
   },
   "source": [
    "    Sing a song of sixpence,\n",
    "    A pocket full of rye.\n",
    "    Four and twenty blackbirds,\n",
    "    Baked in a pie.\n",
    "\n",
    "    When the pie was opened\n",
    "    The birds began to sing;\n",
    "    Wasn’t that a dainty dish,\n",
    "    To set before the king.\n",
    "\n",
    "    The king was in his counting house,\n",
    "    Counting out his money;\n",
    "    The queen was in the parlour,\n",
    "    Eating bread and honey.\n",
    "\n",
    "    The maid was in the garden,\n",
    "    Hanging out the clothes,\n",
    "    When down came a blackbird\n",
    "    And pecked off her nose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdOJmdjU7knW"
   },
   "source": [
    "# Sequence Generation\n",
    "\n",
    "A language model must be trained on the text, and in the case of a character-based language model, the input and output sequences must be characters.\n",
    "\n",
    "The number of characters used as input will also define the number of characters that will need to be provided to the model in order to elicit the first predicted character.\n",
    "\n",
    "After the first character has been generated, it can be appended to the input sequence and used as input for the model to generate the next character.\n",
    "\n",
    "Longer sequences offer more context for the model to learn what character to output next but take longer to train and impose more burden on seeding the model when generating text.\n",
    "\n",
    "We will use an arbitrary length of 10 characters for this model.\n",
    "\n",
    "There is not a lot of text, and 10 characters is a few words.\n",
    "\n",
    "We can now transform the raw text into a form that our model can learn; specifically, input and output sequences of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JjJfvv1U7knY"
   },
   "outputs": [],
   "source": [
    "#load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6GbMmMs7knb",
    "outputId": "7e8c0a51-b9eb-4d22-e49a-65a6777fd8fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sing a song of sixpence,A pocket full of rye.Four and twenty blackbirds,Baked in a pie.When the pie was openedThe birds began to sing;Wasn’t that a dainty dish,To set before the king.The king was in his counting house,Counting out his money;The queen was in the parlour,Eating bread and honey.The maid was in the garden,Hanging out the clothes,When down came a blackbirdAnd pecked off her nose.\n",
      "Total Sequences: 384\n"
     ]
    }
   ],
   "source": [
    "#load text\n",
    "raw_text = load_doc('rhymes.txt')\n",
    "print(raw_text)\n",
    "\n",
    "# clean\n",
    "tokens = raw_text.split()\n",
    "raw_text = ' '.join(tokens)\n",
    "\n",
    "# organize into sequences of characters\n",
    "length = 10\n",
    "sequences = list()\n",
    "for i in range(length, len(raw_text)):\n",
    "    # select sequence of tokens\n",
    "    seq = raw_text[i-length:i+1]\n",
    "    # store\n",
    "    sequences.append(seq)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iBepEyDn7kne"
   },
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'char_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohjVbv1l7kng"
   },
   "source": [
    "# Train a Model\n",
    "In this section, we will develop a neural language model for the prepared sequence data.\n",
    "\n",
    "The model will read encoded characters and predict the next character in the sequence. A Long Short-Term Memory recurrent neural network hidden layer will be used to learn the context from the input sequence in order to make the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7HBgtQvY7knh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jackf\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "r8ruXoPS7knl"
   },
   "outputs": [],
   "source": [
    "# load\n",
    "in_filename = 'char_sequences.txt'\n",
    "raw_text = load_doc(in_filename)\n",
    "lines = raw_text.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTnvoY-f7kno"
   },
   "source": [
    "The sequences of characters must be encoded as integers.This means that each unique character will be assigned a specific integer value and each sequence of characters will be encoded as a sequence of integers. We can create the mapping given a sorted set of unique characters in the raw input data. The mapping is a dictionary of character values to integer values.\n",
    "\n",
    "Next, we can process each sequence of characters one at a time and use the dictionary mapping to look up the integer value for each character. The result is a list of integer lists.\n",
    "\n",
    "We need to know the size of the vocabulary later. We can retrieve this as the size of the dictionary mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kMndzt5v7kno",
    "outputId": "fd9d3ee9-4856-4674-f46e-1e1540e0b2ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 38\n"
     ]
    }
   ],
   "source": [
    "# integer encode sequences of characters\n",
    "chars = sorted(list(set(raw_text)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "sequences = list()\n",
    "for line in lines:\n",
    "    # integer encode line\n",
    "    encoded_seq = [mapping[char] for char in line]\n",
    "    # store\n",
    "    sequences.append(encoded_seq)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(mapping)\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "X = array(sequences)\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qyCICfV7knx"
   },
   "source": [
    "The model is defined with an input layer that takes sequences that have 10 time steps and 38 features for the one hot encoded input sequences. Rather than specify these numbers, we use the second and third dimensions on the X input data. This is so that if we change the length of the sequences or size of the vocabulary, we do not need to change the model definition.\n",
    "\n",
    "The model has a single LSTM hidden layer with 75 memory cells. The model has a fully connected output layer that outputs one vector with a probability distribution across all characters in the vocabulary. A softmax activation function is used on the output layer to ensure the output has the properties of a probability distribution.\n",
    "\n",
    "The model is learning a multi-class classification problem, therefore we use the categorical log loss intended for this type of problem. The efficient Adam implementation of gradient descent is used to optimize the model and accuracy is reported at the end of each batch update. The model is fit for 50 training epochs.\n",
    "\n",
    "# To Do:\n",
    "- Try different numbers of memory cells\n",
    "- Try different types and amounts of recurrent and fully connected layers\n",
    "- Try different lengths of training epochs\n",
    "- Try different sequence lengths and pre-processing of data\n",
    "- Try regularization techniques such as Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1jO9lYX7kny",
    "outputId": "ec627531-d63b-4d3c-cccf-1116a1fce38b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 128)               85504     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 19)                2451      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 38)                760       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88715 (346.54 KB)\n",
      "Trainable params: 88715 (346.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/250\n",
      "12/12 [==============================] - 1s 4ms/step - loss: 3.6185 - accuracy: 0.1198\n",
      "Epoch 2/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.5143 - accuracy: 0.1589\n",
      "Epoch 3/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.3459 - accuracy: 0.1589\n",
      "Epoch 4/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.2350 - accuracy: 0.1589\n",
      "Epoch 5/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.1630 - accuracy: 0.1589\n",
      "Epoch 6/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.1031 - accuracy: 0.1589\n",
      "Epoch 7/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.0628 - accuracy: 0.1589\n",
      "Epoch 8/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3.0286 - accuracy: 0.1589\n",
      "Epoch 9/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.0095 - accuracy: 0.1615\n",
      "Epoch 10/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 2.9612 - accuracy: 0.1589\n",
      "Epoch 11/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.9329 - accuracy: 0.1589\n",
      "Epoch 12/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.9209 - accuracy: 0.1797\n",
      "Epoch 13/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.8837 - accuracy: 0.1719\n",
      "Epoch 14/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 2.8514 - accuracy: 0.2214\n",
      "Epoch 15/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 2.8105 - accuracy: 0.2109\n",
      "Epoch 16/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 2.7738 - accuracy: 0.2214\n",
      "Epoch 17/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.7372 - accuracy: 0.2474\n",
      "Epoch 18/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.6836 - accuracy: 0.2656\n",
      "Epoch 19/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.6545 - accuracy: 0.2500\n",
      "Epoch 20/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.6002 - accuracy: 0.2760\n",
      "Epoch 21/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 2.5576 - accuracy: 0.2812\n",
      "Epoch 22/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 2.5185 - accuracy: 0.2786\n",
      "Epoch 23/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.4721 - accuracy: 0.2943\n",
      "Epoch 24/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.4643 - accuracy: 0.3203\n",
      "Epoch 25/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.4380 - accuracy: 0.3047\n",
      "Epoch 26/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.3844 - accuracy: 0.3151\n",
      "Epoch 27/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.3117 - accuracy: 0.3333\n",
      "Epoch 28/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.2581 - accuracy: 0.3542\n",
      "Epoch 29/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.2235 - accuracy: 0.3802\n",
      "Epoch 30/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 2.1752 - accuracy: 0.3646\n",
      "Epoch 31/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.1456 - accuracy: 0.3906\n",
      "Epoch 32/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.1136 - accuracy: 0.3828\n",
      "Epoch 33/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.0722 - accuracy: 0.4089\n",
      "Epoch 34/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.0221 - accuracy: 0.4375\n",
      "Epoch 35/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 2.0375 - accuracy: 0.4089\n",
      "Epoch 36/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.9629 - accuracy: 0.4427\n",
      "Epoch 37/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.8999 - accuracy: 0.4479\n",
      "Epoch 38/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.8537 - accuracy: 0.4714\n",
      "Epoch 39/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.7842 - accuracy: 0.4870\n",
      "Epoch 40/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.7539 - accuracy: 0.4844\n",
      "Epoch 41/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.7076 - accuracy: 0.5182\n",
      "Epoch 42/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.6722 - accuracy: 0.5156\n",
      "Epoch 43/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.6106 - accuracy: 0.5130\n",
      "Epoch 44/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.5900 - accuracy: 0.5339\n",
      "Epoch 45/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.5441 - accuracy: 0.5547\n",
      "Epoch 46/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.4650 - accuracy: 0.5859\n",
      "Epoch 47/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.4187 - accuracy: 0.6120\n",
      "Epoch 48/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.3546 - accuracy: 0.6354\n",
      "Epoch 49/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2977 - accuracy: 0.6562\n",
      "Epoch 50/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2486 - accuracy: 0.6615\n",
      "Epoch 51/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2101 - accuracy: 0.6849\n",
      "Epoch 52/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1628 - accuracy: 0.7005\n",
      "Epoch 53/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1062 - accuracy: 0.7214\n",
      "Epoch 54/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0340 - accuracy: 0.7422\n",
      "Epoch 55/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9796 - accuracy: 0.7552\n",
      "Epoch 56/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9247 - accuracy: 0.7812\n",
      "Epoch 57/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9223 - accuracy: 0.7812\n",
      "Epoch 58/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8731 - accuracy: 0.7995\n",
      "Epoch 59/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.8236 - accuracy: 0.8021\n",
      "Epoch 60/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.7544 - accuracy: 0.8411\n",
      "Epoch 61/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.7298 - accuracy: 0.8359\n",
      "Epoch 62/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.7031 - accuracy: 0.8516\n",
      "Epoch 63/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6743 - accuracy: 0.8594\n",
      "Epoch 64/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6172 - accuracy: 0.8828\n",
      "Epoch 65/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5706 - accuracy: 0.8828\n",
      "Epoch 66/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5485 - accuracy: 0.8932\n",
      "Epoch 67/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5257 - accuracy: 0.9062\n",
      "Epoch 68/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5081 - accuracy: 0.9141\n",
      "Epoch 69/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4735 - accuracy: 0.9115\n",
      "Epoch 70/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 0.9323\n",
      "Epoch 71/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3964 - accuracy: 0.9401\n",
      "Epoch 72/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3769 - accuracy: 0.9557\n",
      "Epoch 73/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3497 - accuracy: 0.9531\n",
      "Epoch 74/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3238 - accuracy: 0.9661\n",
      "Epoch 75/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.2883 - accuracy: 0.9740\n",
      "Epoch 76/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2601 - accuracy: 0.9766\n",
      "Epoch 77/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.9818\n",
      "Epoch 78/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.9740\n",
      "Epoch 79/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.2148 - accuracy: 0.9818\n",
      "Epoch 80/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.2007 - accuracy: 0.9818\n",
      "Epoch 81/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.1889 - accuracy: 0.9792\n",
      "Epoch 82/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.1703 - accuracy: 0.9896\n",
      "Epoch 83/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1624 - accuracy: 0.9896\n",
      "Epoch 84/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1593 - accuracy: 0.9818\n",
      "Epoch 85/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1494 - accuracy: 0.9896\n",
      "Epoch 86/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1437 - accuracy: 0.9948\n",
      "Epoch 87/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9948\n",
      "Epoch 88/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1171 - accuracy: 0.9948\n",
      "Epoch 89/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1161 - accuracy: 0.9948\n",
      "Epoch 90/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1109 - accuracy: 0.9896\n",
      "Epoch 91/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1054 - accuracy: 0.9896\n",
      "Epoch 92/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.9948\n",
      "Epoch 93/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 0.9948\n",
      "Epoch 94/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9948\n",
      "Epoch 95/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0813 - accuracy: 0.9896\n",
      "Epoch 96/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.9922\n",
      "Epoch 97/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0776 - accuracy: 0.9896\n",
      "Epoch 98/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0757 - accuracy: 0.9948\n",
      "Epoch 99/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9922\n",
      "Epoch 100/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9922\n",
      "Epoch 101/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9922\n",
      "Epoch 102/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0614 - accuracy: 0.9948\n",
      "Epoch 103/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9948\n",
      "Epoch 104/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0555 - accuracy: 0.9922\n",
      "Epoch 105/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0546 - accuracy: 0.9922\n",
      "Epoch 106/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0551 - accuracy: 0.9896\n",
      "Epoch 107/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9948\n",
      "Epoch 108/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.9948\n",
      "Epoch 109/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0443 - accuracy: 0.9922\n",
      "Epoch 110/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0444 - accuracy: 0.9896\n",
      "Epoch 111/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0448 - accuracy: 0.9948\n",
      "Epoch 112/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0445 - accuracy: 0.9922\n",
      "Epoch 113/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0393 - accuracy: 0.9948\n",
      "Epoch 114/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0433 - accuracy: 0.9922\n",
      "Epoch 115/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0392 - accuracy: 0.9922\n",
      "Epoch 116/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0396 - accuracy: 0.9922\n",
      "Epoch 117/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0362 - accuracy: 0.9948\n",
      "Epoch 118/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.9948\n",
      "Epoch 119/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.9896\n",
      "Epoch 120/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0332 - accuracy: 0.9922\n",
      "Epoch 121/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.9896\n",
      "Epoch 122/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.9922\n",
      "Epoch 123/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0311 - accuracy: 0.9922\n",
      "Epoch 124/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 0.9948\n",
      "Epoch 125/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.9948\n",
      "Epoch 126/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.9922\n",
      "Epoch 127/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0287 - accuracy: 0.9922\n",
      "Epoch 128/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0285 - accuracy: 0.9922\n",
      "Epoch 129/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.9922\n",
      "Epoch 130/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.9922\n",
      "Epoch 131/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.9948\n",
      "Epoch 132/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.9948\n",
      "Epoch 133/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.9922\n",
      "Epoch 134/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.9922\n",
      "Epoch 135/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.9948\n",
      "Epoch 136/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.9922\n",
      "Epoch 137/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.9922\n",
      "Epoch 138/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9948\n",
      "Epoch 139/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.9948\n",
      "Epoch 140/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.9896\n",
      "Epoch 141/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9922\n",
      "Epoch 142/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.9948\n",
      "Epoch 143/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.9948\n",
      "Epoch 144/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.9948\n",
      "Epoch 145/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9922\n",
      "Epoch 146/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9922\n",
      "Epoch 147/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.9896\n",
      "Epoch 148/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.9922\n",
      "Epoch 149/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.9948\n",
      "Epoch 150/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9922\n",
      "Epoch 151/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.9896\n",
      "Epoch 152/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.9922\n",
      "Epoch 153/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.9948\n",
      "Epoch 154/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0191 - accuracy: 0.9948\n",
      "Epoch 155/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.9922\n",
      "Epoch 156/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.9896\n",
      "Epoch 157/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.9922\n",
      "Epoch 158/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.9922\n",
      "Epoch 159/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.9948\n",
      "Epoch 160/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.9896\n",
      "Epoch 161/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.9922\n",
      "Epoch 162/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.9922\n",
      "Epoch 163/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.9922\n",
      "Epoch 164/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.9922\n",
      "Epoch 165/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0162 - accuracy: 0.9948\n",
      "Epoch 166/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 0.9922\n",
      "Epoch 167/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0162 - accuracy: 0.9922\n",
      "Epoch 168/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0179 - accuracy: 0.9896\n",
      "Epoch 169/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 0.9922\n",
      "Epoch 170/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0163 - accuracy: 0.9922\n",
      "Epoch 171/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.9948\n",
      "Epoch 172/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0149 - accuracy: 0.9948\n",
      "Epoch 173/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 0.9948\n",
      "Epoch 174/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 0.9948\n",
      "Epoch 175/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0160 - accuracy: 0.9922\n",
      "Epoch 176/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.9922\n",
      "Epoch 177/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 0.9948\n",
      "Epoch 178/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.9948\n",
      "Epoch 179/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 0.9922\n",
      "Epoch 180/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.9922\n",
      "Epoch 181/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 0.9922\n",
      "Epoch 182/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.9896\n",
      "Epoch 183/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0154 - accuracy: 0.9896\n",
      "Epoch 184/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0143 - accuracy: 0.9948\n",
      "Epoch 185/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0143 - accuracy: 0.9948\n",
      "Epoch 186/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 0.9922\n",
      "Epoch 187/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.9896\n",
      "Epoch 188/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 0.9948\n",
      "Epoch 189/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9922\n",
      "Epoch 190/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 0.9948\n",
      "Epoch 191/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9948\n",
      "Epoch 192/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0147 - accuracy: 0.9948\n",
      "Epoch 193/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9922\n",
      "Epoch 194/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0143 - accuracy: 0.9922\n",
      "Epoch 195/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0141 - accuracy: 0.9922\n",
      "Epoch 196/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.9922\n",
      "Epoch 197/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0149 - accuracy: 0.9896\n",
      "Epoch 198/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9948\n",
      "Epoch 199/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9948\n",
      "Epoch 200/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9922\n",
      "Epoch 201/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 0.9922\n",
      "Epoch 202/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9948\n",
      "Epoch 203/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0152 - accuracy: 0.9922\n",
      "Epoch 204/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 0.9896\n",
      "Epoch 205/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 0.9896\n",
      "Epoch 206/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 0.9948\n",
      "Epoch 207/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0123 - accuracy: 0.9922\n",
      "Epoch 208/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.9922\n",
      "Epoch 209/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.9922\n",
      "Epoch 210/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9948\n",
      "Epoch 211/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 0.9922\n",
      "Epoch 212/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9922\n",
      "Epoch 213/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9948\n",
      "Epoch 214/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9922\n",
      "Epoch 215/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9922\n",
      "Epoch 216/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9948\n",
      "Epoch 217/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9948\n",
      "Epoch 218/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9948\n",
      "Epoch 219/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.9922\n",
      "Epoch 220/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.9922\n",
      "Epoch 221/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.9948\n",
      "Epoch 222/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9948\n",
      "Epoch 223/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9948\n",
      "Epoch 224/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.9896\n",
      "Epoch 225/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9948\n",
      "Epoch 226/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9922\n",
      "Epoch 227/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.9948\n",
      "Epoch 228/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0107 - accuracy: 0.9948\n",
      "Epoch 229/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.9922\n",
      "Epoch 230/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9922\n",
      "Epoch 231/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.9948\n",
      "Epoch 232/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9922\n",
      "Epoch 233/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 0.9922\n",
      "Epoch 234/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.9922\n",
      "Epoch 235/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9922\n",
      "Epoch 236/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9922\n",
      "Epoch 237/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.9922\n",
      "Epoch 238/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.9948\n",
      "Epoch 239/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 0.9922\n",
      "Epoch 240/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.9948\n",
      "Epoch 241/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.9896\n",
      "Epoch 242/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9922\n",
      "Epoch 243/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.9922\n",
      "Epoch 244/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9922\n",
      "Epoch 245/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 0.9922\n",
      "Epoch 246/250\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 0.9948\n",
      "Epoch 247/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9896\n",
      "Epoch 248/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9922\n",
      "Epoch 249/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 0.9922\n",
      "Epoch 250/250\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(vocab_size / 2, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(X, y, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raAPR9Qp7kn1",
    "outputId": "8f51d60d-2470-475e-e1b8-089b6c9efcfb"
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the mapping\n",
    "dump(mapping, open('mapping.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv4Q4Rkf7kn4"
   },
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EigunCf57kn6"
   },
   "source": [
    "We must provide sequences of 10 characters as input to the model in order to start the generation process. We will pick these manually. A given input sequence will need to be prepared in the same way as preparing the training data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6UCWctow7kn-"
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # one hot encode\n",
    "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
    "        # predict character\n",
    "        yhat = np.argmax(model.predict(encoded), axis=-1)\n",
    "        # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += char\n",
    "    return in_text\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# load the mapping\n",
    "mapping = load(open('mapping.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXn18EY57koA"
   },
   "source": [
    "Running the example generates three sequences of text.\n",
    "\n",
    "The first is a test to see how the model does at starting from the beginning of the rhyme. The second is a test to see how well it does at beginning in the middle of a line. The final example is a test to see how well it does with a sequence of characters never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5UhgrJYh7koB",
    "outputId": "3eacd528-3048-42a7-fba2-40db0ab0e5b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Sing a song of sixpence,A pock\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "king was in his counting house\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "hello worls.oore eingwou aan h\n"
     ]
    }
   ],
   "source": [
    "# test start of rhyme\n",
    "print(generate_seq(model, mapping, 10, 'Sing a son', 20))\n",
    "# test mid-line\n",
    "print(generate_seq(model, mapping, 10, 'king was i', 20))\n",
    "# test not in original\n",
    "print(generate_seq(model, mapping, 10, 'hello worl', 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXhN-xhG7koE"
   },
   "source": [
    "If the results aren't satisfactory, try out the suggestions above or these below:\n",
    "- Padding. Update the example to provides sequences line by line only and use padding to fill out each sequence to the maximum line length.\n",
    "- Sequence Length. Experiment with different sequence lengths and see how they impact the behavior of the model.\n",
    "- Tune Model. Experiment with different model configurations, such as the number of memory cells and epochs, and try to develop a better model for fewer resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kyXdm-f7koF"
   },
   "source": [
    "# Deliverables to receive credit\n",
    "\n",
    "1. (4 points) Optimize the cells above to tune the model so that it generates text that closely resembles the orginal line from the rhyme, or at least generates sensible words. It's okay if the third example using unseen text still looks somewhat strange  though. Again, this is a toy problem, as language models require a lot of computation. This toy problem is great for rapid experimentation to explore different aspects of deep learning language models.\n",
    "2. (3 points) Write a function to split the text corpus file into training and validation and pipe the validation data into the model.fit() function to be able to track validation error per epoch. Lookup Keras documentation to see how this is handled.\n",
    "3. (3 points) Write a summary (methods and results) in the cells below of the different things you applied. You must include your intuitions behind what did work and what did not work well.\n",
    "4. (Extra Credit 2.5 points) Do something even more interesting. Try a different source text. Train a word-level model. We'll leave it up to your creativity to explore and write a summary of your methods and results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njvIURzq7koG"
   },
   "source": [
    "## Question 1\n",
    "1. The only thing in the code that you can do is to make it better without breaking the code is to simply increase LSTM units. However, I noticed that it may start to overfit after a larger LSTM, so I just doubled it 150 and was fine. I tried to put in early stopping and Validation split, but it made the model predictions even worse. It wasn't even close to predicting anything correctly. So for now only increase the LSTM units is good choice, which I did and added in droput layer and Dense layer which also prevent it from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "bh9kQZnr7koK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0074 - accuracy: 0.9963 - val_loss: 0.9625 - val_accuracy: 0.7672\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.9626 - val_accuracy: 0.7672\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9963 - val_loss: 0.9627 - val_accuracy: 0.7672\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9963 - val_loss: 0.9620 - val_accuracy: 0.7672\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.9963 - val_loss: 0.9628 - val_accuracy: 0.7586\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9963 - val_loss: 0.9616 - val_accuracy: 0.7672\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9963 - val_loss: 0.9617 - val_accuracy: 0.7672\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9611 - val_accuracy: 0.7672\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9925 - val_loss: 0.9610 - val_accuracy: 0.7672\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9620 - val_accuracy: 0.7672\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9963 - val_loss: 0.9621 - val_accuracy: 0.7672\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9963 - val_loss: 0.9611 - val_accuracy: 0.7672\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9963 - val_loss: 0.9614 - val_accuracy: 0.7672\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0069 - accuracy: 0.9963 - val_loss: 0.9623 - val_accuracy: 0.7672\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9963 - val_loss: 0.9623 - val_accuracy: 0.7672\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 0.9644 - val_accuracy: 0.7672\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9963 - val_loss: 0.9635 - val_accuracy: 0.7672\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0068 - accuracy: 0.9925 - val_loss: 0.9626 - val_accuracy: 0.7672\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9963 - val_loss: 0.9625 - val_accuracy: 0.7672\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9925 - val_loss: 0.9617 - val_accuracy: 0.7672\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9963 - val_loss: 0.9639 - val_accuracy: 0.7672\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.9925 - val_loss: 0.9647 - val_accuracy: 0.7672\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9655 - val_accuracy: 0.7672\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9925 - val_loss: 0.9654 - val_accuracy: 0.7672\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9925 - val_loss: 0.9648 - val_accuracy: 0.7672\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9925 - val_loss: 0.9646 - val_accuracy: 0.7672\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9925 - val_loss: 0.9653 - val_accuracy: 0.7672\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9963 - val_loss: 0.9664 - val_accuracy: 0.7672\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9666 - val_accuracy: 0.7672\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9963 - val_loss: 0.9673 - val_accuracy: 0.7672\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9669 - val_accuracy: 0.7672\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9652 - val_accuracy: 0.7672\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9963 - val_loss: 0.9654 - val_accuracy: 0.7672\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9925 - val_loss: 0.9672 - val_accuracy: 0.7672\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 0.9686 - val_accuracy: 0.7672\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9685 - val_accuracy: 0.7672\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9963 - val_loss: 0.9672 - val_accuracy: 0.7672\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9925 - val_loss: 0.9662 - val_accuracy: 0.7672\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9963 - val_loss: 0.9662 - val_accuracy: 0.7672\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9963 - val_loss: 0.9683 - val_accuracy: 0.7672\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9697 - val_accuracy: 0.7672\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0062 - accuracy: 0.9925 - val_loss: 0.9704 - val_accuracy: 0.7672\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9925 - val_loss: 0.9701 - val_accuracy: 0.7672\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9925 - val_loss: 0.9711 - val_accuracy: 0.7672\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9963 - val_loss: 0.9704 - val_accuracy: 0.7672\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9963 - val_loss: 0.9713 - val_accuracy: 0.7672\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.9963 - val_loss: 0.9735 - val_accuracy: 0.7672\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9731 - val_accuracy: 0.7672\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0075 - accuracy: 0.9963 - val_loss: 0.9718 - val_accuracy: 0.7672\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9925 - val_loss: 0.9738 - val_accuracy: 0.7672\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9963 - val_loss: 0.9743 - val_accuracy: 0.7672\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9925 - val_loss: 0.9749 - val_accuracy: 0.7672\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9753 - val_accuracy: 0.7672\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9963 - val_loss: 0.9756 - val_accuracy: 0.7672\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9925 - val_loss: 0.9753 - val_accuracy: 0.7672\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9963 - val_loss: 0.9778 - val_accuracy: 0.7672\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0073 - accuracy: 0.9963 - val_loss: 0.9791 - val_accuracy: 0.7672\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9963 - val_loss: 0.9799 - val_accuracy: 0.7672\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9963 - val_loss: 0.9798 - val_accuracy: 0.7672\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9925 - val_loss: 0.9790 - val_accuracy: 0.7672\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9925 - val_loss: 0.9780 - val_accuracy: 0.7672\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9963 - val_loss: 0.9778 - val_accuracy: 0.7672\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9963 - val_loss: 0.9776 - val_accuracy: 0.7672\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0096 - accuracy: 0.9963 - val_loss: 0.9811 - val_accuracy: 0.7672\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0083 - accuracy: 0.9963 - val_loss: 0.9821 - val_accuracy: 0.7672\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9963 - val_loss: 0.9819 - val_accuracy: 0.7672\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9824 - val_accuracy: 0.7672\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9963 - val_loss: 0.9824 - val_accuracy: 0.7672\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9925 - val_loss: 0.9771 - val_accuracy: 0.7672\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9963 - val_loss: 0.9741 - val_accuracy: 0.7672\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9963 - val_loss: 0.9748 - val_accuracy: 0.7672\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9963 - val_loss: 0.9764 - val_accuracy: 0.7672\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9963 - val_loss: 0.9769 - val_accuracy: 0.7672\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9963 - val_loss: 0.9773 - val_accuracy: 0.7672\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9769 - val_accuracy: 0.7672\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0059 - accuracy: 0.9963 - val_loss: 0.9755 - val_accuracy: 0.7672\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9963 - val_loss: 0.9752 - val_accuracy: 0.7672\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0055 - accuracy: 0.9963 - val_loss: 0.9752 - val_accuracy: 0.7672\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0065 - accuracy: 0.9925 - val_loss: 0.9775 - val_accuracy: 0.7672\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9963 - val_loss: 0.9789 - val_accuracy: 0.7672\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9963 - val_loss: 0.9793 - val_accuracy: 0.7672\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9807 - val_accuracy: 0.7672\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9925 - val_loss: 0.9816 - val_accuracy: 0.7672\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9925 - val_loss: 0.9813 - val_accuracy: 0.7672\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.9963 - val_loss: 0.9806 - val_accuracy: 0.7672\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9963 - val_loss: 0.9817 - val_accuracy: 0.7672\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9925 - val_loss: 0.9810 - val_accuracy: 0.7672\n",
      "Epoch 88/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9925 - val_loss: 0.9821 - val_accuracy: 0.7672\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9963 - val_loss: 0.9810 - val_accuracy: 0.7672\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9963 - val_loss: 0.9797 - val_accuracy: 0.7672\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9963 - val_loss: 0.9805 - val_accuracy: 0.7672\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9963 - val_loss: 0.9824 - val_accuracy: 0.7672\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9841 - val_accuracy: 0.7672\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9925 - val_loss: 0.9854 - val_accuracy: 0.7672\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9925 - val_loss: 0.9849 - val_accuracy: 0.7672\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9963 - val_loss: 0.9829 - val_accuracy: 0.7672\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9963 - val_loss: 0.9825 - val_accuracy: 0.7672\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0071 - accuracy: 0.9963 - val_loss: 0.9829 - val_accuracy: 0.7672\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9925 - val_loss: 0.9855 - val_accuracy: 0.7672\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9963 - val_loss: 0.9858 - val_accuracy: 0.7672\n"
     ]
    }
   ],
   "source": [
    "## Question # 2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def model_split(model, X, y, test_size = 0.30):\n",
    "    train_X, val_X, train_y, val_y = train_test_split(X, y, \n",
    "                                                      test_size = test_size,\n",
    "                                                      random_state = 42)\n",
    "    model.fit(train_X, train_y, epochs = 100, validation_data = (val_X, val_y))\n",
    "model_split(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "The first thing I thought about doing was adding cross validation, to reduce overfitting. This however, made the model predict even worse then before, to where it wasn't even able to predict the line correctly anymore. However, I think I did the cross-validation sets wrong, and there has to be better way of doing it. The second thing I did was try early stopping becuase I noticed that the accuracy would go up and down, as if it was trend, which suggested some sort of overfitting or the model not working accurately, once it hits a plateau it should stop, didn't implement that correctly either. The thing that worked were to add layers, and that helped the accuracy go up by a bit, and didn't seem to be overfitting as the hello world sentence looked as expected. I incrased the n_charts to 50, and anything above it seem to worsen the model, and the same deal when you try to increase the epochs value and LSTM values. It seems that the values perform once after a certain value, maybe higher values suggest overfitting, but increase the values helps. For this we need to write a code to find the best values, but I just ball park the values and just rerunning the code, and settled with the values in the middle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
